{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "from datetime import timedelta, datetime, tzinfo\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import pickle\n",
    "\n",
    "from kafka import KafkaConsumer, TopicPartition\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils, OffsetRange\n",
    "\n",
    "from SharpWriter import SharpWriter\n",
    "from SparkSharpRunner import SparkSharpRunner\n",
    "\n",
    "# make graphics inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define experiment-specific parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sid = 17554\n",
    "\n",
    "sharpWriter = SharpWriter()\n",
    "# pixel size (um), distance (m), wavelength (nm), det_side \n",
    "sharpWriter.init(55, 0.5, 0.083, 100) \n",
    "prbfile = '../../data/17554/recon_17554_probe.npy'\n",
    "cxifile = '../../data/17554/hxn17554.kafka.cxi'\n",
    "\n",
    "sharpRunner = SparkSharpRunner()\n",
    "# print interval, the cxi file\n",
    "args = ['sharp-nsls2', '-o', '10',  cxifile]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Kafka consumer method that reads frames and scan points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data(consumer, n):\n",
    "    \n",
    "    frames = []\n",
    "    xs = []\n",
    "    ys = []   \n",
    "    \n",
    "    for i in range(0, n):\n",
    "        msg = next(consumer)\n",
    "        value = pickle.loads(msg.value)\n",
    "        # print(i, value[0], len(value), msg.offset)\n",
    "        frames.extend(value[1])\n",
    "        xs.extend(value[2])\n",
    "        ys.extend(value[3])\n",
    "        \n",
    "    return frames, np.asarray(xs), np.asarray(ys)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Kafka consumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topic='topic-d'\n",
    "\n",
    "partition = TopicPartition(topic, 0)\n",
    "\n",
    "consumer = KafkaConsumer(\n",
    "    group_id='my-group',\n",
    "    bootstrap_servers=['localhost:9092'],\n",
    "    auto_offset_reset='latest',\n",
    "    enable_auto_commit=False)\n",
    "\n",
    "consumer.assign([partition])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the scan data from the Kafka topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the initial offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2002\n"
     ]
    }
   ],
   "source": [
    "# consumer.seek(partition, 9009)\n",
    "start_offset = consumer.position(partition)\n",
    "print(start_offset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the number of messages of the scan data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic-d:0:2002: n:1000\n"
     ]
    }
   ],
   "source": [
    "msg = next(consumer)\n",
    "value = pickle.loads(msg.value)\n",
    "n = value[0]\n",
    "print(\"%s:%d:%d: n:%d\" % (msg.topic, msg.partition, msg.offset, n) ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get frames and scan points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting frames and scan points from Kafka, updating a file ...\n",
      "processing time:  0:00:07.726523\n"
     ]
    }
   ],
   "source": [
    "print(\"getting frames and scan points from Kafka, updating a file ...\");\n",
    "t1 = datetime.now();\n",
    "frames, xs, ys = get_data(consumer, n)\n",
    "t2 = datetime.now();\n",
    "print (\"processing time: \", (t2 - t1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update the SHARP-NSLS2 input file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update a cxi file ...\n",
      "processing time:  0:00:01.404288\n"
     ]
    }
   ],
   "source": [
    "print(\"update a cxi file ...\");\n",
    "t1 = datetime.now();\n",
    "sharpWriter.write(cxifile, prbfile, frames, xs, ys)\n",
    "t2 = datetime.now();\n",
    "print (\"processing time: \", (t2 - t1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Running the SHARP-NSLS2 MPI/GPU application on four workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running sharp-mpi on spark workers...\n",
      "total processing time:  0:00:31.917395\n",
      "sharp time on each worker: \n",
      "0 initialization + reconstruction time:  0:00:25.322300\n",
      "1 initialization + reconstruction time:  0:00:25.797465\n",
      "2 initialization + reconstruction time:  0:00:25.319605\n",
      "3 initialization + reconstruction time:  0:00:24.772900\n"
     ]
    }
   ],
   "source": [
    "partitions = 4\n",
    "\n",
    "print(\"running sharp-mpi on spark workers...\");\n",
    "t1 = datetime.now();\n",
    "tsharp = sharpRunner.run_with_spark(args, partitions)\n",
    "t2 = datetime.now();\n",
    "print (\"total processing time: \", (t2 - t1))\n",
    "\n",
    "print (\"sharp time on each worker: \")\n",
    "for i in range(0, partitions):\n",
    "    print(i, \"initialization + reconstruction time: \", tsharp[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
